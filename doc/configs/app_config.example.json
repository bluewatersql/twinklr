{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "description": "BlinkB0t Application Configuration - Shared settings for all jobs/tasks",
  
  "output_dir": "artifacts",
  "cache_dir": "data/audio_cache",
  
  "audio_processing": {
    "description": "Audio analysis configuration for feature extraction",
    "hop_length": 512,
    "frame_length": 2048,
    "cache_enabled": true,
    "enhancements": {
      "description": "Audio enhancement features (v3.0) - metadata, lyrics, phonemes",
      "enable_metadata": true,
      "enable_lyrics": true,
      "enable_phonemes": true,
      "enable_acoustid": false,
      "enable_musicbrainz": false,
      "enable_lyrics_lookup": false,
      "enable_whisperx": false,
      "enable_diarization": false,
      "lyrics_language": "en",
      "whisperx_model": "base",
      "whisperx_device": "cpu",
      "_comments": {
        "enable_metadata": "Extract metadata from embedded tags (always) + optional network providers",
        "enable_lyrics": "Resolve lyrics from embedded (.lrc) + optional network lookup + optional WhisperX",
        "enable_phonemes": "Generate phoneme/viseme timing from lyrics (requires timed words)",
        "enable_acoustid": "REQUIRES: ACOUSTID_API_KEY env var + chromaprint binary installed",
        "enable_musicbrainz": "Query MusicBrainz for canonical metadata (rate limited to 1 rps)",
        "enable_lyrics_lookup": "Query online lyrics providers (LRCLib, Genius). REQUIRES: Provider API keys",
        "enable_whisperx": "Use WhisperX AI for lyrics transcription/alignment. REQUIRES: Model download (~150MB for 'base')",
        "enable_diarization": "Detect speakers in audio. REQUIRES: HF_TOKEN env var + Pyannote model (~300MB)",
        "lyrics_language": "Language code for lyrics processing (en, es, fr, etc.)",
        "whisperx_model": "WhisperX model size: tiny(~75MB), base(~150MB), small(~500MB), medium(~1.5GB), large(~3GB)",
        "whisperx_device": "Device for WhisperX: cpu (slow, works everywhere), cuda (fast, NVIDIA GPU), mps (fast, Apple Silicon)",
        "_installation": "Minimal: 'uv sync' | Full ML: 'uv sync --extra ml' | Binary deps: brew install chromaprint ffmpeg"
      }
    },
    "_comments": {
      "hop_length": "Number of samples between successive frames (64-2048). Lower = more detail, higher CPU",
      "frame_length": "Size of FFT window in samples (512-8192). Must be >= hop_length",
      "cache_enabled": "Cache audio analysis results to disk for faster re-runs"
    }
  },
  
  "planning": {
    "description": "LLM context building configuration - controls token budget for audio features",
    "max_beats": 1000,
    "max_energy_points": 1000,
    "max_spectral_points": 50,
    "max_transients": 50,
    "max_sections": 25,
    "_comments": {
      "max_beats": "Maximum beat timestamps to include in LLM context (100-2000)",
      "max_energy_points": "Maximum energy envelope points (100-2000)",
      "max_spectral_points": "Maximum spectral feature points (50-1000)",
      "max_transients": "Maximum transient events to include (5-100)",
      "max_sections": "Maximum structural sections to include (4-50)",
      "_note": "Reduce these values if hitting LLM token limits. Increase for longer songs or more detail."
    }
  },
  
  "logging": {
    "description": "Application logging configuration",
    "level": "INFO",
    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    "_comments": {
      "level": "Log level: DEBUG (verbose), INFO (normal), WARNING, ERROR, CRITICAL",
      "format": "Python logging format string"
    }
  },
  
  "_usage": {
    "description": "How to use this configuration file",
    "location": "Save as 'config.json' in project root",
    "validation": "Validated by AppConfig Pydantic model (packages/blinkb0t/core/config/models.py)",
    "defaults": "All fields shown here are defaults - you can omit fields to use defaults",
    "forward_compatibility": "Unknown fields are ignored for forward compatibility"
  }
}
