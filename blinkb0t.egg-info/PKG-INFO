Metadata-Version: 2.4
Name: blinkb0t
Version: 0.2.0
Summary: BlinkB0t - AI-powered lighting sequencer for xLights
Requires-Python: >=3.12
Description-Content-Type: text/markdown
Requires-Dist: blinkb0t-core>=0.1.0
Requires-Dist: blinkb0t-cli>=0.1.0
Requires-Dist: tqdm>=4.67.1
Provides-Extra: dev
Requires-Dist: pytest>=7.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"
Requires-Dist: mypy>=1.0; extra == "dev"

```text
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â•šâ•â•â–ˆâ–ˆâ•”â•â•â•
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   
â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   
â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•   â–ˆâ–ˆâ•‘   
â•šâ•â•     â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â•    â•šâ•â•   

                        Twinklr
              intelligent choreography for xLights
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ âœˆ
```

# Twinklr

**Twinklr** is an AI-powered choreography engine for **xLights** that transforms music into professional DMX moving head sequences. Using advanced audio analysis, template-based composition, and multi-agent orchestration, it creates synchronized lighting shows that intelligently respond to musical structure, energy, and emotion.

---

## Why Twinklr?

### ğŸ¯ Intelligent Design Philosophy

- **Template Composition**: Multi-step templates compose curated movement, geometry, and dimmer primitivesâ€”no raw DMX generation
- **Semantic Abstraction**: Think in *poses* (FORWARD, AUDIENCE_CENTER) and *intensities* (SMOOTH, DRAMATIC) rather than degrees and DMX values
- **Quality Assurance**: Built-in Plan â†’ Validate â†’ Implement â†’ Judge â†’ Iterate cycle ensures professional results
- **Musical Understanding**: Deep audio analysis captures beats, bars, energy curves, and song structure
- **Separation of Concerns**: Movement choreography (templates) stays independent from appearance (channels)

### âœ¨ Key Capabilities

**Audio Intelligence**
- Beat-perfect timing with high-accuracy beat and bar detection
- Multi-scale energy analysis across frequency ranges
- Automatic structure detection (intro, verse, chorus, bridge, outro)
- Tempo, key, time signature, and harmonic analysis
- Smart caching for instant iteration

**Template System**
- 30+ curated movement patterns (sweeps, circles, waves, figure-8s, etc.)
- 10+ spatial geometry transforms (mirror, fan, wave, chevron, tunnel)
- 15+ dimmer behaviors (pulse, breathe, swell, fade, strobe)
- Multi-step choreographies with beat-aware transitions
- Pose abstractions independent of fixture configuration

**Multi-Agent Orchestration**
- Strategic planning stage selects appropriate templates
- Heuristic validation catches timing and structural errors
- Implementation stage assigns fixtures and creates variations
- Quality evaluation scores across diversity, alignment, and creativity
- Iterative refinement until quality threshold met

**DMX Channel Support**
- Pan and tilt with value curves and transitions
- Dimmer control with categorical intensity levels
- Shutter effects (strobe, pulse, random)
- Color preset coordination
- Gobo pattern selection

---

## Quick Start

### Prerequisites

- **Python 3.12+** (required)
- **uv** package manager (recommended) or pip
- **xLights** - For viewing and editing generated sequences ([xlights.org](https://xlights.org/))
- **OpenAI API Key** - For AI-powered choreography planning

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd blinkb0t

# Install dependencies with Make (recommended)
make install

# Or install with uv directly
uv sync --extra dev --all-packages

# Or use pip (development mode)
pip install -e packages/blinkb0t/core -e packages/blinkb0t/cli
```

### Configuration

Create a `.env` file in the repository root:

```bash
OPENAI_API_KEY=sk-your-api-key-here
```

Edit configuration files to match your setup:

- **`config.json`**: Application settings (cache directories, logging, LLM models)
- **`job_config.json`**: Job settings (iterations, thresholds, feature flags)
- **`fixture_config.json`**: Your moving head fixture definitions and DMX mapping

### Run Your First Sequence

Use the included example files to test the system:

```bash
uv run blinkb0t run \
  --audio "data/music/Need A Favor.mp3" \
  --xsq "data/sequences/Need A Favor.xsq" \
  --config "job_config.json" \
  --out "artifacts"
```

Output files are written to `artifacts/<project_name>/`:
- `song_features.json` - Audio analysis results
- `sequence_fingerprint.json` - Existing effect analysis
- `plan.json` - Generated choreography plan
- `<project_name>_blinkb0t_mh.xsq` - **Your final sequence!**

Open the generated `.xsq` file in xLights to view your AI-generated moving head choreography!

---

## How It Works

Twinklr processes your music through a sophisticated pipeline orchestrated by `MovingHeadManager`:

### 1. Audio Analysis
- Extracts beats, bars, and musical timing using librosa
- Analyzes energy levels across frequency ranges
- Detects song structure (verse, chorus, bridge)
- Identifies tempo, key, and harmonic content
- Results cached in `data/audio_cache/` for instant re-runs

### 2. Sequence Fingerprinting
- Analyzes existing xLights effects in your `.xsq`
- Extracts color palette and timing density
- Builds context for intelligent planning
- Ensures AI complements existing effects

### 3. Multi-Stage Orchestration

The `AgentOrchestrator` runs a quality-assured planning cycle:

**Plan Generation** (`plan_generator.py`)
- LLM analyzes song structure and energy
- Selects appropriate templates for each section
- Assigns categorical parameters (intensity, speed)
- Defines section boundaries and poses

**Heuristic Validation** (`heuristic_validator.py`)
- Fast, deterministic checks (no LLM calls)
- Verifies timing completeness and alignment
- Catches structural errors
- Returns actionable feedback

**Implementation Expansion** (`implementation_expander.py`)
- LLM adds fixture assignments
- Creates variations for repeated sections (chorus 1 vs 2 vs 3)
- Layers accent patterns
- Completes all implementation details

**Quality Evaluation** (`judge_critic.py`)
- Scores plan across multiple dimensions:
  - Template diversity (variety across sections)
  - Musical alignment (energy correlation, timing)
  - Technical correctness (valid references, no gaps)
  - Creativity (variations, thematic coherence)
- Provides detailed feedback

**Refinement Loop** (`refinement_agent.py`)
- If quality threshold not met: iterate
- Incorporates judge feedback
- Continues until success or token budget exhausted
- Guarantees complete, evaluated output

### 4. Template Processing
- Loads selected templates from library
- Resolves categorical parameters to curve settings
- Converts semantic poses to fixture-specific angles
- Expands multi-step choreographies
- Applies beat-aware transitions

### 5. Sequence Generation
- Renders templates to DMX effects
- Handles all enabled channels (pan, tilt, dimmer, shutter, color, gobo)
- Applies channel overlays for appearance
- Generates xLights-compatible XML
- Validates output for correctness

---

## Configuration

### Application Config (`config.json`)

System-wide settings:

```json
{
  "cache_dir": "data/audio_cache",
  "output_dir": "artifacts",
  
  "audio_processing": {
    "hop_length": 512,
    "frame_length": 2048,
    "cache_enabled": true
  },
  
  "planning": {
    "max_beats": 600,
    "max_energy_points": 768,
    "max_sections": 12
  },
  
  "logging": {
    "level": "INFO",
    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  }
}
```

### Job Config (`job_config.json`)

Task-specific settings:

```json
{
  "schema_version": "2.0",
  
  "assumptions": {
    "beats_per_bar": 4
  },
  
  "fixture_config_path": "fixture_config.json",
  
  "agent": {
    "success_threshold": 70,
    "max_iterations": 3,
    "token_budget": 50000
  },
  
  "include_notes_track": true,
  "debug": true,
  "checkpoint": true
}
```

### Fixture Config (`fixture_config.json`)

Define your moving head fixtures:

```json
{
  "moving_heads": {
    "heads": ["MH1", "MH2", "MH3", "MH4"],
    "model_map": {
      "MH1": "MovingHead-1",
      "MH2": "MovingHead-2",
      "MH3": "MovingHead-3",
      "MH4": "MovingHead-4"
    },
    "dmx_mapping": {
      "pan_channel": 1,
      "tilt_channel": 3,
      "dimmer_channel": 7,
      "shutter_channel": 5,
      "color_channel": 9,
      "gobo_channel": 11
    },
    "pan_tilt_range": {
      "pan_range_deg": 540.0,
      "tilt_range_deg": 270.0
    },
    "orientation": {
      "pan_front_dmx": 128,
      "tilt_zero_dmx": 0,
      "tilt_up_dmx": 255
    }
  }
}
```

**Key Options:**
- `success_threshold`: Minimum judge score (0-100) to accept plan
- `max_iterations`: Refinement cycles allowed (0 = no judge, single pass)
- `token_budget`: Maximum tokens across all agent stages
- `checkpoint`: Enable checkpoint reuse for faster debugging

---

## Architecture

Twinklr is built as a modern monorepo with clean separation of concerns:

### Core Package (`blinkb0t-core`)

```
packages/blinkb0t/core/
â”œâ”€â”€ agents/                    # Multi-Agent Orchestration
â”‚   â””â”€â”€ moving_heads/         
â”‚       â”œâ”€â”€ orchestrator.py   # Main orchestration loop
â”‚       â”œâ”€â”€ context_shaper.py # Token-efficient context prep
â”‚       â”œâ”€â”€ plan_generator.py # Strategic planning stage
â”‚       â”œâ”€â”€ heuristic_validator.py # Fast validation
â”‚       â”œâ”€â”€ implementation_expander.py # Detail stage
â”‚       â”œâ”€â”€ judge_critic.py   # Quality evaluation
â”‚       â””â”€â”€ refinement_agent.py # Iteration logic
â”œâ”€â”€ domains/                   # Domain Logic
â”‚   â”œâ”€â”€ audio/                # Music analysis
â”‚   â”‚   â”œâ”€â”€ processor.py     # Audio processing
â”‚   â”‚   â””â”€â”€ analyzer.py      # Feature extraction
â”‚   â””â”€â”€ sequencing/           # Sequence generation
â”‚       â”œâ”€â”€ timing/           # Musical time abstraction
â”‚       â”œâ”€â”€ curves/           # Value curve generation
â”‚       â”œâ”€â”€ channels/         # DMX channel handlers
â”‚       â”‚   â”œâ”€â”€ handlers/    # Shutter, color, gobo
â”‚       â”‚   â””â”€â”€ libraries/   # Channel pattern libraries
â”‚       â”œâ”€â”€ moving_heads/     # Moving head sequencing
â”‚       â”‚   â”œâ”€â”€ templates/   # Template system
â”‚       â”‚   â”œâ”€â”€ poses/       # Pose abstraction
â”‚       â”‚   â”œâ”€â”€ libraries/   # Movement/geometry/dimmer
â”‚       â”‚   â”œâ”€â”€ manager.py   # Pipeline coordinator
â”‚       â”‚   â””â”€â”€ sequencer.py # Rendering engine
â”‚       â””â”€â”€ xsq/             # xLights file format
â”œâ”€â”€ models/                   # Pydantic Data Models
â”‚   â”œâ”€â”€ templates.py         # Template definitions
â”‚   â”œâ”€â”€ poses.py             # Pose models
â”‚   â”œâ”€â”€ orchestration.py     # Agent models
â”‚   â””â”€â”€ timing.py            # Timing models
â”œâ”€â”€ config/                   # Configuration
â”‚   â””â”€â”€ models.py            # Config models
â”œâ”€â”€ api/                      # External Services
â”‚   â””â”€â”€ openai/              # OpenAI wrapper
â””â”€â”€ utils/                    # Utilities
    â”œâ”€â”€ json.py              # JSON helpers
    â””â”€â”€ math.py              # Math utilities
```

### CLI Package (`blinkb0t-cli`)

```
packages/blinkb0t/cli/
â””â”€â”€ main.py                   # Command-line interface
```

### Design Principles

1. **Separation of Concerns**: Templates (movement) and channels (appearance) are independent
2. **Template Composition**: Multi-step templates compose curated primitives
3. **LLM at Right Level**: Plans templates and categorical parameters, never raw DMX
4. **Required Quality Loop**: Plan â†’ Validate â†’ Implement â†’ Judge â†’ Iterate
5. **Type Safety**: Pydantic models and MyPy strict mode throughout
6. **Testability**: 80%+ coverage with comprehensive unit/integration tests

---

## Development

### Project Structure

```
blinkb0t/
â”œâ”€â”€ packages/                 # Monorepo packages
â”‚   â””â”€â”€ blinkb0t/
â”‚       â”œâ”€â”€ core/            # Core business logic
â”‚       â””â”€â”€ cli/             # Command-line interface
â”œâ”€â”€ tests/                    # Test suite (80%+ coverage)
â”‚   â”œâ”€â”€ unit/               # Unit tests
â”‚   â”œâ”€â”€ integration/        # Integration tests
â”‚   â””â”€â”€ e2e/                # End-to-end tests
â”œâ”€â”€ data/                     # Data and caches
â”‚   â”œâ”€â”€ music/              # Sample audio files
â”‚   â”œâ”€â”€ sequences/          # Sample xLights sequences
â”‚   â”œâ”€â”€ audio_cache/        # Analysis cache
â”‚   â””â”€â”€ v2/templates/       # Template library
â”œâ”€â”€ artifacts/                # Generated outputs
â”œâ”€â”€ changes/                  # Architecture docs
â”‚   â”œâ”€â”€ done/               # Completed changes
â”‚   â””â”€â”€ future/             # Planned enhancements
â”œâ”€â”€ scripts/                  # Utility scripts
â”œâ”€â”€ .cursorrules             # Development guidelines
â”œâ”€â”€ pyproject.toml           # Workspace config
â”œâ”€â”€ Makefile                 # Development automation
â””â”€â”€ README.md                # This file
```

### Quality Standards

- **Type Safety**: MyPy strict mode, 100% type hint coverage
- **Code Quality**: Ruff linting and formatting (100 char line length)
- **Test Coverage**: Minimum 80% requirement, currently 65%+
- **TDD Approach**: Write tests before implementation
- **Clean Architecture**: Dependency injection, separation of concerns
- **Documentation**: Comprehensive docstrings using Google style

See [`.cursorrules`](.cursorrules) for complete development guidelines.

### Running Tests

```bash
# Using Makefile (recommended)
make test              # Run all tests
make test-cov          # Run with HTML coverage report
make test-unit         # Run unit tests only
make test-integration  # Run integration tests

# Or use pytest directly
uv run pytest
uv run pytest --cov=blinkb0t.core --cov-report=html
uv run pytest tests/unit/
uv run pytest -v -k "test_template"
```

### Code Quality

```bash
# Run all checks (recommended before commit)
make check-all

# Individual checks
make lint              # Ruff linting
make format            # Auto-format code
make type-check        # MyPy type checking

# Or run tools directly
uv run ruff check .
uv run ruff format .
uv run mypy packages/blinkb0t/core
```

### Development Workflow

1. **Create feature branch**: `git checkout -b feature/your-feature`
2. **Write tests**: Start with test cases (TDD approach)
3. **Implement**: Write code with complete type hints
4. **Run checks**: `make check-all` before committing
5. **Ensure coverage**: Maintain 80%+ test coverage
6. **Commit**: Use conventional commit messages
7. **Create PR**: Request review before merging

---

## Examples

### Demo Script

A demo script is provided for quick testing:

```bash
python scripts/demo.py
```

### Custom Movement Patterns

The movement, geometry, and dimmer libraries are defined in Python modules with type-safe enums:

```python
# packages/blinkb0t/core/domains/sequencing/moving_heads/libraries/movements.py

from enum import Enum
from pydantic import BaseModel

class MovementID(str, Enum):
    SWEEP_LR = "sweep_lr"
    CIRCLE = "circle"
    FIGURE8 = "figure8"
    # ... 30+ patterns

class MovementPattern(BaseModel):
    id: MovementID
    name: str
    primary_curve: CurveMapping
    categorical_params: dict[CategoricalIntensity, CategoricalParams]
```

Templates are stored as JSON files in `data/v2/templates/` and validated against the Python libraries.

---

## Strategy & Architecture Docs

For deeper architectural context and vision:

- **`changes/done/rewrite/strategy_v2.md`** - Complete strategic architecture
- **`changes/done/rewrite/IMPLEMENTATION_ROADMAP.md`** - 12-week implementation plan
- **`changes/done/rewrite/DESIGN_PACKAGE_SUMMARY.md`** - Design package overview
- **`changes/done/rewrite/QUICK_START.md`** - Quick start guide

---

## Troubleshooting

### No API Key Error

If you see an error about missing API key:

```bash
# Create .env file in repository root
echo "OPENAI_API_KEY=sk-your-key-here" > .env
```

### Audio Processing Errors

Ensure audio dependencies are installed:

```bash
uv sync --extra dev --all-packages
```

### xLights Sequence Issues

- Ensure your `.xsq` file is valid xLights XML
- Check that moving head models in `fixture_config.json` match your xLights sequence
- Verify DMX channel mappings match your fixture specifications
- Use xLights' built-in validator to check the generated sequence

### Template Validation Errors

- Templates must reference valid movement/geometry/dimmer IDs from Python libraries
- Check `data/v2/templates/` for template JSON structure
- Ensure categorical parameters use valid intensity levels (SMOOTH, MEDIUM, DRAMATIC, INTENSE, EXTREME)

---

## Contributing

We welcome contributions! Please follow these guidelines:

1. **Read the guidelines**: See [`.cursorrules`](.cursorrules) for development standards
2. **Write tests**: Follow TDD approach with 80%+ coverage
3. **Run quality checks**: `make check-all` before committing
4. **Type everything**: Complete type hints required
5. **Document changes**: Update architecture docs in `changes/` for significant changes
6. **Use conventional commits**: Follow conventional commit message format

---

## License

TBD

---

## Acknowledgments

Built with:
- **[xLights](https://xlights.org/)** - Professional lighting sequencer
- **[OpenAI](https://openai.com/)** - AI/LLM capabilities
- **[librosa](https://librosa.org/)** - Audio analysis
- **[Pydantic](https://pydantic.dev/)** - Data validation and settings management

---

**Twinklr** â€” Transforming music into intelligent light âœ¨
